<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog of Ricky</title><link>https://qige96.github.io/myblog/</link><description>Recent content on Blog of Ricky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 31 Jul 2022 22:39:49 +0100</lastBuildDate><atom:link href="https://qige96.github.io/myblog/index.xml" rel="self" type="application/rss+xml"/><item><title>A Review of Probabilistic Logic: SRL - SLP</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-slp/</link><pubDate>Sun, 31 Jul 2022 22:39:49 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-slp/</guid><description>The last SRL theory I would like to introduce here is Stochasitc Logic Program1 (SLP).
Similar to Problog, Stochastic Logic Programs is also a probabilistic extesion of classic logic programs. However, SLP is different from Problog, in particular the semantics. So far, the probabilistic logic we had introduced are based on possible world semantics. That is, the probability value assigned to a proposition indicates “the probability of the proposition being true”.</description></item><item><title>A Review of Probabilistic Logic: SRL - Problog</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-problog/</link><pubDate>Sun, 31 Jul 2022 22:35:02 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-problog/</guid><description>Problog1 is a probabilistic extension to the classical logic programming language Prolog.
A problog program is a set of probabilistic facts and a set of deterministic rules. A probabilistic fact is denoted as \(p::\phi\) where \(\phi\) is an atom and \(p\) is the probability. A rule is denoted as \(h :- b_1,...,b_k\) where the head \(h\) and body \(b_i\) are all atoms. A rule says that \(h\) is true only if all the \(b\) are true.</description></item><item><title>A Review of Probabilistic Logic: SRL - Intro</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-intro/</link><pubDate>Sun, 31 Jul 2022 22:12:48 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-intro/</guid><description>In this part, we introduce a bunch of theories combining logic and probability that emerged in the late 1990s, called Statistical Relational Learning (SRL). As the name indicates, SRL studies how to learn and exploit the statistical structure among relational data (or tabular data, a set of data in the form of a table).
Recall that the central question in probabilistic logic is \(\phi_1^{X_1},\dots,\phi_n^{X_n}|\!\!\!\approx \psi^Y\), and the key challenge is handling dependency relationships.</description></item><item><title>A Review of Probabilistic Logic: GOFPL - Bacchus</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bacchus/</link><pubDate>Sun, 31 Jul 2022 20:46:07 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bacchus/</guid><description>So far, we reviewed two probabilistic logic theories: Nilsson’s Probabilistic Logic and Bundy’s Incidence Calculus. However, within these logics, we can only use probability at a meta-level, but cannot talk about probability within the logic. Like example 2.1, with the three propositions \(\{rainy, sunny, windy\}\), we can only talk about \(\neg windy \land rainy\) or \((sunny \lor windy) \land \neg rainy\), and compute their probabilities, but not the relations between the probabilities of them, such as “the probability of rainy is less than that of sunny”.</description></item><item><title>A Review of Probabilistic Logic: GOFPL - Bundy</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bundy/</link><pubDate>Sun, 31 Jul 2022 20:44:42 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bundy/</guid><description>In the last section, we saw Nilsson proposed his probabilistic logic by assigning probabilities to possible worlds, and then assigning possible worlds to propositions. Therefore, identifying dependency relationships among propositions can be reduced to set operations of possible worlds. Nearly meanwhile, Alan Bundy leveraged the same idea to design his mechanism for probabilistic reasoning: Incidence Calculus. Here the term “incidence” is Bundy’s name for the possible world. In fact, the first formal paper on Incidence Calculus was published in 1985, even one year ahead of Nilsson’s formal paper on Probabilistic Logic.</description></item><item><title>A Review of Probabilistic Logic: GOFPL - Nilsson</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-nilsson/</link><pubDate>Sun, 31 Jul 2022 20:44:36 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-nilsson/</guid><description>Nils Nilsson is the first person to coin the name Probabilistic Logic1 (In the rest of this note, I use the term “Probabilistic Logic” to indicate the particular theory of Nilsson, and “probabilistic logic” to refer to the general class of probabilistic deductive inference mechanism). His theory has also been considered the seminal work of probabilistic reasoning in Artificial Intelligence that sparked a lot of subsequent works.
Nilsson’s proposed method focuses on logical sentences, i.</description></item><item><title>A Review of Probabilistic Logic: Preface</title><link>https://qige96.github.io/myblog/posts/review-probabilistic-logic/</link><pubDate>Sun, 31 Jul 2022 16:34:57 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-probabilistic-logic/</guid><description>Introduction There are different kinds of Logic. Deductive Logic tells us how to derive the absolute truth of the conclusion given the premises. Inductive logic. Here the term probabilistic logic specifically refers to probabilistic deductive logic.
Why do we study probabilistic logic? We know that by deduction, we can tell the truth of the conclusion given the truth of the premises. But how do we know the truth of the premises?</description></item><item><title>Explaining Probability Calibration</title><link>https://qige96.github.io/myblog/posts/explaining-probability-calibration/</link><pubDate>Sat, 30 Jul 2022 00:27:00 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/explaining-probability-calibration/</guid><description>This is a reading note explaining what probability calibration is, why we should use it, and how to achieve it. I will firstly introduce the origin of this notion, and then explain how to achieve probability calibration and why this can work. Lastly, I reveal the benefits of probability calibration.
Philosophical Origin What is probability calibration? In a word, calibration means the forecast probabilities match the relative frequencies: \(fr(X|pr(X)=\beta)=\beta\), where \(fr(X)\) represents the relative frequency of \(X\) and \(pr(X)\) represents the predicted probability of \(X\).</description></item><item><title>5G Network Summary</title><link>https://qige96.github.io/myblog/posts/5g-network-summary/</link><pubDate>Tue, 26 Jul 2022 16:12:58 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/5g-network-summary/</guid><description>This note is a summary of some key points and comments to the description of 5G network architecture, which is defined by European Telecommunications Standards Institute. (ETSI).
Network Architecture: Traditional versus Cloud Native We all know that a computer network is a group of inter-connected computers, among which computers could communicate with one another.
Traditionally, we need some devices to facilitate the networking ability, typically switches and routers, which are THE two key devices for computer network to forward messages.</description></item></channel></rss>