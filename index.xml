<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog of Ricky</title><link>https://qige96.github.io/myblog/</link><description>Recent content on Blog of Ricky</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 09 Oct 2022 15:01:31 +0100</lastBuildDate><atom:link href="https://qige96.github.io/myblog/index.xml" rel="self" type="application/rss+xml"/><item><title>A Review of Probabilistic Logic: NeSy - LNN</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-nesy-lnn/</link><pubDate>Sun, 09 Oct 2022 15:01:31 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-nesy-lnn/</guid><description>Logical Neural Network1 is proposed by IBM in 2020. The key ideas of LNN are simple:
Extend the truth value of logical formula from \(\{True, False\}\) to real numbers. The authors called this extended logic weighted real-value logic. Typically, when the truth values range within \([0, 1]\), they can serve as probabilities. For each logical formula, create a neural net, so that evaluating the truth value of a logical formula is equivalent to evaluate a neural net.</description></item><item><title>A Review of Probabilistic Logic: NeSy - Intro</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-nesy-intro/</link><pubDate>Thu, 25 Aug 2022 15:30:42 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-nesy-intro/</guid><description>Fairly simply speaking, Neuro-Symbolic AI, or NeSy for short, refers to the AI approach that combines artificial neural networks and symbolic reasoners. NeSy research has a long history. In recent years, because of the profound success of deep neural networks in solving various AI problems, NeSy also receives great attention, as many people believe that by proper methods, the combination of the two approaches can achieve better performance than any of the pure approaches.</description></item><item><title>A Review of Probabilistic Logic: SRL - SLP</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-slp/</link><pubDate>Sun, 31 Jul 2022 22:39:49 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-slp/</guid><description>The last SRL theory I would like to introduce here is Stochasitc Logic Program1 (SLP).
Similar to Problog, Stochastic Logic Programs is also a probabilistic extesion of classic logic programs. However, SLP is different from Problog, in particular the semantics. So far, the probabilistic logic we had introduced are based on possible world semantics. That is, the probability value assigned to a proposition indicates “the probability of the proposition being true”.</description></item><item><title>A Review of Probabilistic Logic: SRL - Problog</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-problog/</link><pubDate>Sun, 31 Jul 2022 22:35:02 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-problog/</guid><description>Problog1 is a probabilistic extension to the classical logic programming language Prolog.
A problog program is a set of probabilistic facts and a set of deterministic rules. A probabilistic fact is denoted as \(p::\phi\) where \(\phi\) is an atom and \(p\) is the probability. A rule is denoted as \(h :- b_1,...,b_k\) where the head \(h\) and body \(b_i\) are all atoms. A rule says that \(h\) is true only if all the \(b\) are true.</description></item><item><title>A Review of Probabilistic Logic: SRL - MLN</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-mln/</link><pubDate>Sun, 31 Jul 2022 22:17:26 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-mln/</guid><description>Markov Logic Network1 (MLN) allows users to write down a set of first-order formulas, and assign weights to every formula. These weighted formulas then serve as a set of templates to automatically construct a Markov Network, on which probabilistic inference can be performed.
Since MLN works on first-order logic, concepts including constants, variables, predicates, functions, literals, formulas, and sentences come back. Readers can go back to Bachuss’ Lp for a refresh.</description></item><item><title>A Review of Probabilistic Logic: SRL - Intro</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-srl-intro/</link><pubDate>Sun, 31 Jul 2022 22:12:48 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-srl-intro/</guid><description>In this part, we introduce a bunch of theories combining logic and probability that emerged in the late 1990s, called Statistical Relational Learning (SRL). As the name indicates, SRL studies how to learn and exploit the statistical structure among relational data (or tabular data, a set of data in the form of a table).
Recall that the central question in probabilistic logic is \(\phi_1^{X_1},\dots,\phi_n^{X_n}|\!\!\!\approx \psi^Y\), and the key challenge is handling dependency relationships.</description></item><item><title>A Review of Probabilistic Logic: GOFPL - Bacchus</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bacchus/</link><pubDate>Sun, 31 Jul 2022 20:46:07 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bacchus/</guid><description>So far, we reviewed two probabilistic logic theories: Nilsson’s Probabilistic Logic and Bundy’s Incidence Calculus. However, within these logics, we can only use probability at a meta-level, but cannot talk about probability within the logic. Like example 2.1, with the three propositions \(\{rainy, sunny, windy\}\), we can only talk about \(\neg windy \land rainy\) or \((sunny \lor windy) \land \neg rainy\), and compute their probabilities, but not the relations between the probabilities of them, such as “the probability of rainy is less than that of sunny”.</description></item><item><title>A Review of Probabilistic Logic: GOFPL - Bundy</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bundy/</link><pubDate>Sun, 31 Jul 2022 20:44:42 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-bundy/</guid><description>In the last section, we saw Nilsson proposed his probabilistic logic by assigning probabilities to possible worlds, and then assigning possible worlds to propositions. Therefore, identifying dependency relationships among propositions can be reduced to set operations of possible worlds. Nearly meanwhile, Alan Bundy leveraged the same idea to design his mechanism for probabilistic reasoning: Incidence Calculus. Here the term “incidence” is Bundy’s name for the possible world. In fact, the first formal paper on Incidence Calculus was published in 19851, even one year ahead of Nilsson’s formal paper on Probabilistic Logic.</description></item><item><title>A Review of Probabilistic Logic: GOFPL - Nilsson</title><link>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-nilsson/</link><pubDate>Sun, 31 Jul 2022 20:44:36 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-prob-logic-gofpl-nilsson/</guid><description>Nils Nilsson is the first person to coin the name Probabilistic Logic1 (In the rest of this note, I use the term “Probabilistic Logic” to indicate the particular theory of Nilsson, and “probabilistic logic” to refer to the general class of probabilistic deductive inference mechanism). His theory has also been considered the seminal work of probabilistic reasoning in Artificial Intelligence that sparked a lot of subsequent works.
Nilsson’s proposed method focuses on logical sentences, i.</description></item><item><title>A Review of Probabilistic Logic: Preface</title><link>https://qige96.github.io/myblog/posts/review-probabilistic-logic/</link><pubDate>Sun, 31 Jul 2022 16:34:57 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/review-probabilistic-logic/</guid><description>Introduction There are different kinds of Logic. Deductive Logic tells us how to derive the absolute truth of the conclusion given the premises. Inductive logic. Here the term probabilistic logic specifically refers to probabilistic deductive logic.
Why do we study probabilistic logic? We know that by deduction, we can tell the truth of the conclusion given the truth of the premises. But how do we know the truth of the premises?</description></item><item><title>Explaining Probability Calibration</title><link>https://qige96.github.io/myblog/posts/explaining-probability-calibration/</link><pubDate>Sat, 30 Jul 2022 00:27:00 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/explaining-probability-calibration/</guid><description>This is a reading note explaining what probability calibration is, why we should use it, and how to achieve it. I will firstly introduce the origin of this notion, and then explain how to achieve probability calibration and why this can work. Lastly, I reveal the benefits of probability calibration.
Philosophical Origin What is probability calibration? In a word, calibration means the forecast probabilities match the relative frequencies: \(fr(X|pr(X)=\beta)=\beta\), where \(fr(X)\) represents the relative frequency of \(X\) and \(pr(X)\) represents the predicted probability of \(X\).</description></item><item><title>5G Network Summary</title><link>https://qige96.github.io/myblog/posts/5g-network-summary/</link><pubDate>Tue, 26 Jul 2022 16:12:58 +0100</pubDate><guid>https://qige96.github.io/myblog/posts/5g-network-summary/</guid><description>This note is a summary of some key points and comments to the description of 5G network architecture, which is defined by European Telecommunications Standards Institute. (ETSI).
Network Architecture: Traditional versus Cloud Native We all know that a computer network is a group of inter-connected computers, among which computers could communicate with one another.
Traditionally, we need some devices to facilitate the networking ability, typically switches and routers, which are THE two key devices for computer network to forward messages.</description></item></channel></rss>